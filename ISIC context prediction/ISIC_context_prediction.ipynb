{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ISIC context prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP_jqZGWujaD"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.io import read_image\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "\n",
        "class ISICDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, img_dir, labels_dir, patch_dim, gap):\n",
        "        self.patch_dim, self.gap = patch_dim, gap\n",
        "        file = open(labels_dir, \"r\")\n",
        "        csv_reader = csv.reader(file)\n",
        "\n",
        "        self.img_labels = []\n",
        "        for row in csv_reader:\n",
        "            self.img_labels.append(row)\n",
        "        \n",
        "        self.img_dir = img_dir\n",
        "        \n",
        "        self.transform = transforms.Compose([\n",
        "                        transforms.Resize((700,700)), \n",
        "                        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "                                             ])\n",
        "        \n",
        "    def get_patch_from_grid(self, image, patch_dim, gap):\n",
        "        image = np.array(image)\n",
        "        \n",
        "        offset_x, offset_y = image.shape[0] - (patch_dim*3 + gap*2), image.shape[1] - (patch_dim*3 + gap*2)\n",
        "        start_grid_x, start_grid_y = np.random.randint(0, offset_x), np.random.randint(0, offset_y)\n",
        "        patch_loc_arr = [(1, 1), (1, 2), (1, 3), (2, 1), (2, 3), (3, 1), (3, 2), (3, 3)]\n",
        "        loc = np.random.randint(len(patch_loc_arr))\n",
        "        tempx, tempy = patch_loc_arr[loc]\n",
        "    \n",
        "        patch_x_pt = start_grid_x + patch_dim * (tempx-1) + gap * (tempx-1)\n",
        "        patch_y_pt = start_grid_y + patch_dim * (tempy-1) + gap * (tempy-1)\n",
        "        random_patch = image[patch_x_pt:patch_x_pt+patch_dim, patch_y_pt:patch_y_pt+patch_dim]\n",
        "\n",
        "        patch_x_pt = start_grid_x + patch_dim * (2-1) + gap * (2-1)\n",
        "        patch_y_pt = start_grid_y + patch_dim * (2-1) + gap * (2-1)\n",
        "        uniform_patch = image[patch_x_pt:patch_x_pt+patch_dim, patch_y_pt:patch_y_pt+patch_dim]\n",
        "    \n",
        "        random_patch_label = loc\n",
        "    \n",
        "        return uniform_patch, random_patch, random_patch_label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        path = os.path.join(self.img_dir, self.img_labels[index][0])\n",
        "        image = self.transform(read_image(path)/255).permute(1,2,0)\n",
        "\n",
        "        uniform_patch, random_patch, random_patch_label = self.get_patch_from_grid(image, self.patch_dim, self.gap)\n",
        "        return torch.Tensor(uniform_patch).permute(2,0,1), torch.Tensor(random_patch).permute(2,0,1), random_patch_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMGCNNnJurzX"
      },
      "source": [
        "traindir = '../input/isic-2018/ISIC2018_Task1-2_Training_Input/ISIC2018_Task1-2_Training_Input'\n",
        "train_labels_dir = '../input/isic-2018/train_labels.csv'\n",
        "validdir = '../input/isic-2018/ISIC2018_Task1-2_Validation_Input/ISIC2018_Task1-2_Validation_Input'\n",
        "valid_labels_dir = '../input/isic-2018/valid_labels.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRxTgr7tuuSd"
      },
      "source": [
        "data = ISICDataset(traindir, train_labels_dir, patch_dim=224, gap=10)\n",
        "train_dl = DataLoader(data, batch_size=32, shuffle=True, pin_memory = True)\n",
        "\n",
        "valid_data = ISICDataset(validdir, valid_labels_dir, patch_dim=224, gap=10)\n",
        "valid_dl = DataLoader(valid_data, batch_size = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RMbLdysuwjW"
      },
      "source": [
        "from torchvision.models import resnet34\n",
        "from torch import nn\n",
        "\n",
        "class Resnet34Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Resnet34Network, self).__init__()\n",
        "        res = resnet34()\n",
        "        res.fc = nn.Linear(in_features = 512, out_features = 4096, bias = True)\n",
        "        self.cnn = res\n",
        "        \n",
        "        self.fc = nn.Sequential(nn.Linear(2 * 4096, 4096),\n",
        "                                nn.ReLU(inplace=True), nn.Linear(4096, 4096),\n",
        "                                nn.ReLU(inplace=True), nn.Linear(4096, 8))\n",
        "    \n",
        "    def forward(self, input1, input2):\n",
        "        output1 = self.cnn(input1)\n",
        "        output2 = self.cnn(input2)\n",
        "        output = torch.cat((output1, output2), 1)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "model = Resnet34Network()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngr5jt4Gu0F4"
      },
      "source": [
        "import wandb\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "my_key = UserSecretsClient().get_secret(\"wandb-key\")\n",
        "wandb.login(key = my_key)\n",
        "wandb.init(project='ISIC2018', entity='tro2vs')\n",
        "config = wandb.config\n",
        "config.learning_rate = 1e-5\n",
        "wandb.watch(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqE08nU_u4vs"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import time\n",
        "\n",
        "def norm_pred(pred):\n",
        "    return pred.argmax(dim = 1)\n",
        "\n",
        "def acc(labels, pred):\n",
        "    return (pred.argmax(dim = 1) == labels.cuda()).sum()/len(labels)\n",
        "\n",
        "def train(model, train_dl, valid_dl, loss_fn, optimizer, epochs=1):\n",
        "    start = time.time()\n",
        "    model.cuda()\n",
        "    best_acc = 0\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'valid']:\n",
        "            if phase == 'train':\n",
        "                model.train(True)  \n",
        "                dataloader = train_dl\n",
        "            else:\n",
        "                model.train(False)\n",
        "                dataloader = valid_dl\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_acc = 0.0\n",
        "\n",
        "            step = 0\n",
        "            \n",
        "            batches = 0\n",
        "            \n",
        "            f1 = [0 for i in range(8)]\n",
        "            f1_mic = 0\n",
        "            f1_mac = 0\n",
        "            f1_weighted = 0\n",
        "            \n",
        "            for x1, x2, y in dataloader:\n",
        "                x1 = x1.cuda()\n",
        "                x2 = x2.cuda()\n",
        "                y = y.cuda()\n",
        "                step += 1\n",
        "\n",
        "                if phase == 'train':\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(x1,x2)\n",
        "                    loss = loss_fn(outputs, y)\n",
        "\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                else:\n",
        "                    with torch.no_grad():\n",
        "                        outputs = model(x1,x2)\n",
        "                        loss = loss_fn(outputs, y)\n",
        "                \n",
        "                \n",
        "                f1_0 = f1_score(y.cpu().detach().numpy(),\n",
        "                                            norm_pred(outputs).cpu().detach().numpy(),\n",
        "                                  average = None, labels=[0,1,2,3,4,5,6,7])\n",
        "                \n",
        "                f1 = [f1_0[i]+f1[i] for i in range(8)]\n",
        "                \n",
        "                f1_mic += f1_score(y.cpu().detach().numpy(),\n",
        "                                            norm_pred(outputs).cpu().detach().numpy(),\n",
        "                                  average = 'micro', labels=[0,1,2,3,4,5,6,7])\n",
        "                f1_mac += f1_score(y.cpu().detach().numpy(),\n",
        "                                            norm_pred(outputs).cpu().detach().numpy(),\n",
        "                                  average = 'macro', labels=[0,1,2,3,4,5,6,7])\n",
        "                f1_weighted += f1_score(y.cpu().detach().numpy(),\n",
        "                                            norm_pred(outputs).cpu().detach().numpy(),\n",
        "                                  average = 'weighted', labels=[0,1,2,3,4,5,6,7])\n",
        "                \n",
        "                running_acc  += acc(y, outputs)*dataloader.batch_size\n",
        "                running_loss += loss*dataloader.batch_size\n",
        "                batches += dataloader.batch_size\n",
        "\n",
        "                if step % 10 == 0:\n",
        "                    print('Current step: {}  Loss: {}  Acc: {}  AllocMem (Mb): {}'.format(step, loss, acc(y, outputs), torch.cuda.memory_allocated()/1024/1024))\n",
        "                \n",
        "          \n",
        "                \n",
        "            epoch_loss = running_loss / batches\n",
        "            epoch_acc = running_acc / batches\n",
        "            \n",
        "            print('{} Loss: {:.4f} Acc: {}'.format(phase, epoch_loss, epoch_acc))\n",
        "            \n",
        "            if phase == 'valid':\n",
        "                wandb.log({\"Loss/test\": epoch_loss})\n",
        "                wandb.log({\"Accuracy/test\": epoch_acc})\n",
        "                wandb.log({\"f1-score/test/micro\": f1_mic/step})\n",
        "                wandb.log({\"f1-score/test/macro\": f1_mac/step})\n",
        "                wandb.log({\"f1-score/test/weighted\": f1_weighted/step})\n",
        "\n",
        "                for i in range(8):\n",
        "                    wandb.log({\"f1-score/test/class_\"+str(i): f1[i]/step})      \n",
        "                \n",
        "                if epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    torch.save(model.state_dict(), \"best_model.pt\")\n",
        "                    wandb.save('./best_model.pt')\n",
        "            else:\n",
        "                torch.save(model.state_dict(), \"full_train_model.pt\")\n",
        "                wandb.save('./full_train_model.pt')\n",
        "                \n",
        "                wandb.log({\"Loss/train\": epoch_loss})\n",
        "                wandb.log({\"Accuracy/train\": epoch_acc})\n",
        "                wandb.log({\"f1-score/train/micro\": f1_mic/step})\n",
        "                wandb.log({\"f1-score/train/macro\": f1_mac/step})\n",
        "                wandb.log({\"f1-score/train/weighted\": f1_weighted/step})\n",
        "                \n",
        "                for i in range(8):\n",
        "                    wandb.log({\"f1-score/train/class_\"+str(i): f1[i]/step})\n",
        "                \n",
        "\n",
        "\n",
        "    time_elapsed = time.time() - start\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n",
        "    #model = torch.load(\"best_model.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df3CEWwGvNRq"
      },
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "opt = torch.optim.Adam(model.parameters(), lr = 1e-5)\n",
        "train(model, train_dl, valid_dl, loss_fn, \n",
        "                             opt, epochs = 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2BPHIMZvQi4"
      },
      "source": [
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}