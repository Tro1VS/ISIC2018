{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom torchvision.io import read_image\nfrom torchvision import transforms\nfrom random import randrange\nimport torch\nimport os\nimport csv\n\n\nclass ISICDataset(Dataset):\n    def __init__(self, img_dir, labels_dir):\n        file = open(labels_dir, \"r\")\n        csv_reader = csv.reader(file)\n\n        self.img_labels = []\n        for row in csv_reader:\n            self.img_labels.append(row)\n        \n        self.img_dir = img_dir\n        self.transform = transforms.Compose([\n                        transforms.Resize((224,224)), \n                        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n                                             ])\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels[idx][0])\n        label = randrange(4)\n        image = torch.rot90(self.transform(read_image(img_path)/255).permute(1,2,0), label, [0,1]).permute(2,0,1)\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2021-08-11T08:12:05.999249Z","iopub.execute_input":"2021-08-11T08:12:05.999579Z","iopub.status.idle":"2021-08-11T08:12:06.014408Z","shell.execute_reply.started":"2021-08-11T08:12:05.999547Z","shell.execute_reply":"2021-08-11T08:12:06.013361Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"traindir = '../input/isic-2018/ISIC2018_Task1-2_Training_Input/ISIC2018_Task1-2_Training_Input'\ntrain_labels_dir = '../input/isic-2018/train_labels.csv'\nvaliddir = '../input/isic-2018/ISIC2018_Task1-2_Validation_Input/ISIC2018_Task1-2_Validation_Input'\nvalid_labels_dir = '../input/isic-2018/valid_labels.csv'","metadata":{"execution":{"iopub.status.busy":"2021-08-11T08:12:08.832033Z","iopub.execute_input":"2021-08-11T08:12:08.832392Z","iopub.status.idle":"2021-08-11T08:12:08.839070Z","shell.execute_reply.started":"2021-08-11T08:12:08.832338Z","shell.execute_reply":"2021-08-11T08:12:08.836980Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import random_split, DataLoader\n\ndata = ISICDataset(traindir, train_labels_dir)\ntrain_dl = DataLoader(data, batch_size = 32, shuffle = True, pin_memory = True)\n\nvalid_data = ISICDataset(validdir, valid_labels_dir)\nvalid_dl = DataLoader(valid_data, batch_size = 10)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T08:12:11.244800Z","iopub.execute_input":"2021-08-11T08:12:11.245159Z","iopub.status.idle":"2021-08-11T08:12:11.261901Z","shell.execute_reply.started":"2021-08-11T08:12:11.245126Z","shell.execute_reply":"2021-08-11T08:12:11.261066Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import torchvision\nimport torch\nmodel = torchvision.models.resnet34(pretrained = True)\nmodel.fc = torch.nn.Linear(in_features = 512, out_features = 4, bias = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T08:10:42.225394Z","iopub.execute_input":"2021-08-11T08:10:42.225714Z","iopub.status.idle":"2021-08-11T08:10:45.050907Z","shell.execute_reply.started":"2021-08-11T08:10:42.225682Z","shell.execute_reply":"2021-08-11T08:10:45.050026Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/83.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d037bd9c7bfc472c9c3e45ff3cb7cfec"}},"metadata":{}}]},{"cell_type":"code","source":"import wandb\nwandb.init(project='ISIC2018', entity='tro2vs')\nconfig = wandb.config\nconfig.learning_rate = 0.001\nwandb.watch(model)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T08:10:48.835280Z","iopub.execute_input":"2021-08-11T08:10:48.835593Z","iopub.status.idle":"2021-08-11T08:11:05.965641Z","shell.execute_reply.started":"2021-08-11T08:10:48.835563Z","shell.execute_reply":"2021-08-11T08:11:05.964707Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                Tracking run with wandb version 0.10.33<br/>\n                Syncing run <strong style=\"color:#cdcd00\">driven-snowball-30</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/tro2vs/ISIC2018\" target=\"_blank\">https://wandb.ai/tro2vs/ISIC2018</a><br/>\n                Run page: <a href=\"https://wandb.ai/tro2vs/ISIC2018/runs/tuj7kh3c\" target=\"_blank\">https://wandb.ai/tro2vs/ISIC2018/runs/tuj7kh3c</a><br/>\n                Run data is saved locally in <code>/kaggle/working/wandb/run-20210811_081059-tuj7kh3c</code><br/><br/>\n            "},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"[<wandb.wandb_torch.TorchGraph at 0x7ff692a96110>]"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nimport time\n\ndef norm_pred(pred):\n    return pred.argmax(dim = 1)\n\ndef acc(labels, pred):\n    return (pred.argmax(dim = 1) == labels.cuda()).sum()/len(labels)\n\ndef train(model, train_dl, valid_dl, loss_fn, optimizer, epochs=1):\n    start = time.time()\n    model.cuda()\n    best_acc = 0\n    \n    for epoch in range(epochs):\n        print('Epoch {}/{}'.format(epoch, epochs - 1))\n        print('-' * 10)\n\n        for phase in ['train', 'valid']:\n            if phase == 'train':\n                model.train(True)  \n                dataloader = train_dl\n            else:\n                model.train(False)\n                dataloader = valid_dl\n\n            running_loss = 0.0\n            running_acc = 0.0\n\n            step = 0\n            \n            for x, y in dataloader:\n                x = x.cuda()\n                y = y.cuda()\n                step += 1\n\n                if phase == 'train':\n                    optimizer.zero_grad()\n                    outputs = model(x)\n                    loss = loss_fn(outputs, y)\n\n                    loss.backward()\n                    optimizer.step()\n\n                else:\n                    with torch.no_grad():\n                        outputs = model(x)\n                        loss = loss_fn(outputs, y)\n\n                running_acc  += acc(y, outputs)*dataloader.batch_size\n                running_loss += loss*dataloader.batch_size\n\n                if step % 10 == 0:\n                    print('Current step: {}  Loss: {}  Acc: {}  AllocMem (Mb): {}'.format(step, loss, acc(y, outputs), torch.cuda.memory_allocated()/1024/1024))\n                \n          \n                \n            epoch_loss = running_loss / len(dataloader.dataset)\n            epoch_acc = running_acc / len(dataloader.dataset)\n            \n            f1 = f1_score(y.cpu().detach().numpy(),\n                                            norm_pred(outputs).cpu().detach().numpy(),\n                                  average = None, labels=[0,1,2,3])\n            f1_mic = f1_score(y.cpu().detach().numpy(),\n                                            norm_pred(outputs).cpu().detach().numpy(),\n                                  average = 'micro', labels=[0,1,2,3])\n            f1_mac = f1_score(y.cpu().detach().numpy(),\n                                            norm_pred(outputs).cpu().detach().numpy(),\n                                  average = 'macro', labels=[0,1,2,3])\n            f1_weighted = f1_score(y.cpu().detach().numpy(),\n                                            norm_pred(outputs).cpu().detach().numpy(),\n                                  average = 'weighted', labels=[0,1,2,3])\n            \n            print('{} Loss: {:.4f} Acc: {}'.format(phase, epoch_loss, epoch_acc))\n            \n            if phase == 'valid':\n                wandb.log({\"Loss/test\": epoch_loss})\n                wandb.log({\"Accuracy/test\": epoch_acc})\n                wandb.log({\"f1-score/test/micro\": f1_mic})\n                wandb.log({\"f1-score/test/macro\": f1_mac})\n                wandb.log({\"f1-score/test/weighted\": f1_weighted})\n\n                for i in range(4):\n                    wandb.log({\"f1-score/test/class_\"+str(i): f1[i]})      \n                \n                if epoch_acc > best_acc:\n                    best_acc = epoch_acc\n                    torch.save(model.state_dict(), \"best_model.pt\")\n                    wandb.save('./best_model.pt')\n            else:\n                torch.save(model.state_dict(), \"full_train_model.pt\")\n                wandb.save('./full_train_model.pt')\n                \n                wandb.log({\"Loss/train\": epoch_loss})\n                wandb.log({\"Accuracy/train\": epoch_acc})\n                wandb.log({\"f1-score/train/micro\": f1_mic})\n                wandb.log({\"f1-score/train/macro\": f1_mac})\n                wandb.log({\"f1-score/train/weighted\": f1_weighted})\n                \n                for i in range(4):\n                    wandb.log({\"f1-score/train/class_\"+str(i): f1[i]})\n                \n\n\n    time_elapsed = time.time() - start\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n    #model = torch.load(\"best_model.pt\")","metadata":{"execution":{"iopub.status.busy":"2021-08-11T08:12:19.875667Z","iopub.execute_input":"2021-08-11T08:12:19.876062Z","iopub.status.idle":"2021-08-11T08:12:20.449718Z","shell.execute_reply.started":"2021-08-11T08:12:19.876029Z","shell.execute_reply":"2021-08-11T08:12:20.448915Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"loss_fn = torch.nn.CrossEntropyLoss()\nopt = torch.optim.Adam(model.parameters(), lr = 0.001)\ntrain(model, train_dl, valid_dl, loss_fn, \n                             opt, epochs = 5)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T08:12:32.232783Z","iopub.execute_input":"2021-08-11T08:12:32.233150Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 0/4\n----------\nCurrent step: 10  Loss: 1.0130443572998047  Acc: 0.53125  AllocMem (Mb): 350.7900390625\nCurrent step: 20  Loss: 0.924130916595459  Acc: 0.5625  AllocMem (Mb): 350.7900390625\nCurrent step: 30  Loss: 1.047688364982605  Acc: 0.46875  AllocMem (Mb): 350.7900390625\nCurrent step: 40  Loss: 0.9951361417770386  Acc: 0.53125  AllocMem (Mb): 350.7900390625\nCurrent step: 50  Loss: 0.6506425738334656  Acc: 0.78125  AllocMem (Mb): 350.7900390625\nCurrent step: 60  Loss: 0.7788416743278503  Acc: 0.5625  AllocMem (Mb): 350.7900390625\nCurrent step: 70  Loss: 0.8680523633956909  Acc: 0.625  AllocMem (Mb): 350.7900390625\nCurrent step: 80  Loss: 0.8818572759628296  Acc: 0.53125  AllocMem (Mb): 350.7900390625\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1465: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n  average, \"true nor predicted\", 'F-score is', len(true_sum)\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.9484 Acc: 0.5585967302322388\nCurrent step: 10  Loss: 0.9828091859817505  Acc: 0.5  AllocMem (Mb): 338.158203125\nvalid Loss: 1.4291 Acc: 0.3199999928474426\nEpoch 1/4\n----------\nCurrent step: 10  Loss: 0.7989829778671265  Acc: 0.46875  AllocMem (Mb): 350.791015625\nCurrent step: 20  Loss: 0.7561054229736328  Acc: 0.65625  AllocMem (Mb): 350.791015625\nCurrent step: 30  Loss: 0.5772686004638672  Acc: 0.71875  AllocMem (Mb): 350.791015625\nCurrent step: 40  Loss: 0.8610515594482422  Acc: 0.53125  AllocMem (Mb): 350.791015625\nCurrent step: 50  Loss: 0.5069336891174316  Acc: 0.6875  AllocMem (Mb): 350.791015625\nCurrent step: 60  Loss: 0.8431142568588257  Acc: 0.59375  AllocMem (Mb): 350.791015625\nCurrent step: 70  Loss: 0.6888386011123657  Acc: 0.625  AllocMem (Mb): 350.791015625\nCurrent step: 80  Loss: 0.8853840827941895  Acc: 0.5  AllocMem (Mb): 350.791015625\ntrain Loss: 0.7636 Acc: 0.6195065379142761\nCurrent step: 10  Loss: 1.299412488937378  Acc: 0.6000000238418579  AllocMem (Mb): 338.15869140625\nvalid Loss: 1.7113 Acc: 0.3799999952316284\nEpoch 2/4\n----------\nCurrent step: 10  Loss: 0.7557783126831055  Acc: 0.625  AllocMem (Mb): 350.791015625\nCurrent step: 20  Loss: 0.7132444977760315  Acc: 0.59375  AllocMem (Mb): 350.791015625\n","output_type":"stream"}]}]}