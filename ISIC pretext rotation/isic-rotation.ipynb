{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom torchvision.io import read_image\nfrom torchvision import transforms\nfrom random import randrange\nimport torch\nimport os\nimport csv\n\n\nclass ISICDataset(Dataset):\n    def __init__(self, img_dir, labels_dir):\n        file = open(labels_dir, \"r\")\n        csv_reader = csv.reader(file)\n\n        self.img_labels = []\n        for row in csv_reader:\n            self.img_labels.append(row)\n        \n        self.img_dir = img_dir\n        self.transform = transforms.Compose([\n                        transforms.Resize((224,224)), \n                        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n                                             ])\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels[idx][0])\n        label = randrange(4)\n        image = torch.rot90(self.transform(read_image(img_path)/255).permute(1,2,0), label, [0,1]).permute(2,0,1)\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:08:09.701198Z","iopub.execute_input":"2021-08-14T14:08:09.701569Z","iopub.status.idle":"2021-08-14T14:08:11.169255Z","shell.execute_reply.started":"2021-08-14T14:08:09.701484Z","shell.execute_reply":"2021-08-14T14:08:11.168407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traindir = '../input/isic-2018/ISIC2018_Task1-2_Training_Input/ISIC2018_Task1-2_Training_Input'\ntrain_labels_dir = '../input/isic-2018/train_labels.csv'\nvaliddir = '../input/isic-2018/ISIC2018_Task1-2_Validation_Input/ISIC2018_Task1-2_Validation_Input'\nvalid_labels_dir = '../input/isic-2018/valid_labels.csv'","metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:08:11.172594Z","iopub.execute_input":"2021-08-14T14:08:11.172847Z","iopub.status.idle":"2021-08-14T14:08:11.178853Z","shell.execute_reply.started":"2021-08-14T14:08:11.172822Z","shell.execute_reply":"2021-08-14T14:08:11.178067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import random_split, DataLoader\n\ndata = ISICDataset(traindir, train_labels_dir)\ntrain_dl = DataLoader(data, batch_size = 32, shuffle = True, pin_memory = True)\n\nvalid_data = ISICDataset(validdir, valid_labels_dir)\nvalid_dl = DataLoader(valid_data, batch_size = 10)","metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:08:11.181653Z","iopub.execute_input":"2021-08-14T14:08:11.181996Z","iopub.status.idle":"2021-08-14T14:08:11.206907Z","shell.execute_reply.started":"2021-08-14T14:08:11.181962Z","shell.execute_reply":"2021-08-14T14:08:11.206114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision\nimport torch\nmodel = torchvision.models.resnet34(pretrained = True)\nmodel.fc = torch.nn.Linear(in_features = 512, out_features = 4, bias = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:08:11.208368Z","iopub.execute_input":"2021-08-14T14:08:11.208758Z","iopub.status.idle":"2021-08-14T14:08:18.559489Z","shell.execute_reply.started":"2021-08-14T14:08:11.208719Z","shell.execute_reply":"2021-08-14T14:08:18.558578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nwandb.init(project='ISIC2018', entity='tro2vs')\nconfig = wandb.config\nconfig.learning_rate = 1e-5\nwandb.watch(model)","metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:08:18.560852Z","iopub.execute_input":"2021-08-14T14:08:18.561443Z","iopub.status.idle":"2021-08-14T14:08:37.201718Z","shell.execute_reply.started":"2021-08-14T14:08:18.561395Z","shell.execute_reply":"2021-08-14T14:08:37.200741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nimport time\n\ndef norm_pred(pred):\n    return pred.argmax(dim = 1)\n\ndef acc(labels, pred):\n    return (pred.argmax(dim = 1) == labels.cuda()).sum()/len(labels)\n\ndef train(model, train_dl, valid_dl, loss_fn, optimizer, epochs=1):\n    start = time.time()\n    model.cuda()\n    best_acc = 0\n    \n    for epoch in range(epochs):\n        print('Epoch {}/{}'.format(epoch, epochs - 1))\n        print('-' * 10)\n\n        for phase in ['train', 'valid']:\n            if phase == 'train':\n                model.train(True)  \n                dataloader = train_dl\n            else:\n                model.train(False)\n                dataloader = valid_dl\n\n            running_loss = 0.0\n            running_acc = 0.0\n\n            step = 0\n            \n            batches = 0\n            \n            f1 = [0 for i in range(4)]\n            f1_mic = 0\n            f1_mac = 0\n            f1_weighted = 0\n            \n            for x, y in dataloader:\n                x = x.cuda()\n                y = y.cuda()\n                step += 1\n\n                if phase == 'train':\n                    optimizer.zero_grad()\n                    outputs = model(x)\n                    loss = loss_fn(outputs, y)\n\n                    loss.backward()\n                    optimizer.step()\n\n                else:\n                    with torch.no_grad():\n                        outputs = model(x)\n                        loss = loss_fn(outputs, y)\n                \n                \n                f1_0 = f1_score(y.cpu().detach().numpy(),\n                                            norm_pred(outputs).cpu().detach().numpy(),\n                                  average = None, labels=[0,1,2,3])\n                \n                f1 = [f1_0[i]+f1[i] for i in range(4)]\n                \n                f1_mic += f1_score(y.cpu().detach().numpy(),\n                                            norm_pred(outputs).cpu().detach().numpy(),\n                                  average = 'micro', labels=[0,1,2,3])\n                f1_mac += f1_score(y.cpu().detach().numpy(),\n                                            norm_pred(outputs).cpu().detach().numpy(),\n                                  average = 'macro', labels=[0,1,2,3])\n                f1_weighted += f1_score(y.cpu().detach().numpy(),\n                                            norm_pred(outputs).cpu().detach().numpy(),\n                                  average = 'weighted', labels=[0,1,2,3])\n                \n                running_acc  += acc(y, outputs)*dataloader.batch_size\n                running_loss += loss*dataloader.batch_size\n                batches += dataloader.batch_size\n\n                if step % 10 == 0:\n                    print('Current step: {}  Loss: {}  Acc: {}  AllocMem (Mb): {}'.format(step, loss, acc(y, outputs), torch.cuda.memory_allocated()/1024/1024))\n                \n          \n                \n            epoch_loss = running_loss / batches\n            epoch_acc = running_acc / batches\n            \n            print('{} Loss: {:.4f} Acc: {}'.format(phase, epoch_loss, epoch_acc))\n            \n            if phase == 'valid':\n                wandb.log({\"Loss/test\": epoch_loss})\n                wandb.log({\"Accuracy/test\": epoch_acc})\n                wandb.log({\"f1-score/test/micro\": f1_mic})\n                wandb.log({\"f1-score/test/macro\": f1_mac})\n                wandb.log({\"f1-score/test/weighted\": f1_weighted})\n\n                for i in range(4):\n                    wandb.log({\"f1-score/test/class_\"+str(i): f1[i]})      \n                \n                if epoch_acc > best_acc:\n                    best_acc = epoch_acc\n                    torch.save(model.state_dict(), \"best_model.pt\")\n                    wandb.save('./best_model.pt')\n            else:\n                torch.save(model.state_dict(), \"full_train_model.pt\")\n                wandb.save('./full_train_model.pt')\n                \n                wandb.log({\"Loss/train\": epoch_loss})\n                wandb.log({\"Accuracy/train\": epoch_acc})\n                wandb.log({\"f1-score/train/micro\": f1_mic})\n                wandb.log({\"f1-score/train/macro\": f1_mac})\n                wandb.log({\"f1-score/train/weighted\": f1_weighted})\n                \n                for i in range(4):\n                    wandb.log({\"f1-score/train/class_\"+str(i): f1[i]})\n                \n\n\n    time_elapsed = time.time() - start\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n    #model = torch.load(\"best_model.pt\")","metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:08:40.161334Z","iopub.execute_input":"2021-08-14T14:08:40.161767Z","iopub.status.idle":"2021-08-14T14:08:40.858713Z","shell.execute_reply.started":"2021-08-14T14:08:40.161724Z","shell.execute_reply":"2021-08-14T14:08:40.857682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_fn = torch.nn.CrossEntropyLoss()\nopt = torch.optim.Adam(model.parameters(), lr = 1e-5)\ntrain(model, train_dl, valid_dl, loss_fn, \n                             opt, epochs = 30)","metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:10:01.095564Z","iopub.execute_input":"2021-08-14T14:10:01.0959Z","iopub.status.idle":"2021-08-14T21:53:24.5333Z","shell.execute_reply.started":"2021-08-14T14:10:01.095869Z","shell.execute_reply":"2021-08-14T21:53:24.530206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T21:55:54.888523Z","iopub.execute_input":"2021-08-14T21:55:54.888976Z","iopub.status.idle":"2021-08-14T21:56:01.121033Z","shell.execute_reply.started":"2021-08-14T21:55:54.888934Z","shell.execute_reply":"2021-08-14T21:56:01.120245Z"},"trusted":true},"execution_count":null,"outputs":[]}]}